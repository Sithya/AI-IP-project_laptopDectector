"""
Compare metrics between original and cleaned dataset models.

This script collects and compares performance metrics from both training runs.
"""

import json
from pathlib import Path
from typing import Dict


def extract_metrics_from_results(results_dir: Path) -> Dict[str, float]:
    """
    Extract metrics from YOLO training results.
    
    Args:
        results_dir: Path to YOLO training results directory
    
    Returns:
        Dictionary with metrics
    """
    metrics = {}
    
    # Try to read from results.json if it exists
    results_json = results_dir / "results.json"
    if results_json.exists():
        with results_json.open("r") as f:
            data = json.load(f)
            # Extract final epoch metrics
            if data:
                final_epoch = data[-1]
                metrics = {
                    "precision": final_epoch.get("metrics/precision(B)", 0),
                    "recall": final_epoch.get("metrics/recall(B)", 0),
                    "mAP50": final_epoch.get("metrics/mAP50(B)", 0),
                    "mAP50-95": final_epoch.get("metrics/mAP50-95(B)", 0),
                }
    
    # Also check for validation results
    val_results = results_dir / "val" / "results.json"
    if val_results.exists():
        with val_results.open("r") as f:
            data = json.load(f)
            if data:
                metrics.update({
                    "val_precision": data.get("precision", 0),
                    "val_recall": data.get("recall", 0),
                    "val_mAP50": data.get("mAP50", 0),
                    "val_mAP50-95": data.get("mAP50-95", 0),
                })
    
    return metrics


def compare_models():
    """Compare metrics between original and cleaned models."""
    project_root = Path(__file__).resolve().parent
    
    original_dir = project_root / "runs" / "yolov8n-laptop"
    cleaned_dir = project_root / "runs" / "yolov8n-laptop-cleaned"
    
    print("=" * 80)
    print("MODEL PERFORMANCE COMPARISON")
    print("=" * 80)
    print()
    
    # Get metrics (we'll use the test set results from terminal output)
    # Since YOLO doesn't always save test metrics easily, we'll use what we know
    
    print("ðŸ“Š TEST SET METRICS COMPARISON")
    print("-" * 80)
    print(f"{'Metric':<20} {'Original Dataset':<20} {'Cleaned Dataset':<20} {'Difference':<20}")
    print("-" * 80)
    
    # Original dataset metrics (from your training output)
    original_metrics = {
        "Precision": 0.911,
        "Recall": 0.783,
        "mAP@0.5": 0.871,
        "mAP@0.5:0.95": 0.594,
        "Test Images": 225,
        "Test Instances": 356,
    }
    
    # Cleaned dataset metrics (from your training output)
    cleaned_metrics = {
        "Precision": 0.885,
        "Recall": 0.745,
        "mAP@0.5": 0.835,
        "mAP@0.5:0.95": 0.535,
        "Test Images": 204,
        "Test Instances": 337,
    }
    
    for metric in ["Precision", "Recall", "mAP@0.5", "mAP@0.5:0.95"]:
        orig_val = original_metrics[metric]
        clean_val = cleaned_metrics[metric]
        diff = clean_val - orig_val
        diff_pct = (diff / orig_val) * 100 if orig_val > 0 else 0
        
        sign = "+" if diff >= 0 else ""
        print(f"{metric:<20} {orig_val:<20.3f} {clean_val:<20.3f} {sign}{diff:.3f} ({sign}{diff_pct:.1f}%)")
    
    print("-" * 80)
    print()
    
    print("ðŸ“ˆ DATASET STATISTICS")
    print("-" * 80)
    print(f"{'Statistic':<30} {'Original':<20} {'Cleaned':<20}")
    print("-" * 80)
    print(f"{'Total Images':<30} {1500:<20} {1351:<20}")
    print(f"{'Train Images':<30} {1050:<20} {945:<20}")
    print(f"{'Val Images':<30} {225:<20} {202:<20}")
    print(f"{'Test Images':<30} {225:<20} {204:<20}")
    print(f"{'Test Instances':<30} {356:<20} {337:<20}")
    print("-" * 80)
    print()
    
    print("ðŸ” DATA CLEANING SUMMARY")
    print("-" * 80)
    print("Removed:")
    print("  - Invalid bounding boxes: 136")
    print("  - Too dark images: 8")
    print("  - Too bright images: 5")
    print("  - Duplicates: 0")
    print("  - Missing labels: 0")
    print("  Total removed: 149 images (9.9%)")
    print()
    print("Enhancements applied:")
    print("  - Brightness adjustment (factor: 1.1)")
    print("  - Contrast adjustment (factor: 1.1)")
    print("  - Sharpening filter")
    print("  Applied to: 1,351 images (100% of kept images)")
    print("-" * 80)
    print()
    
    print("ðŸ“ KEY FINDINGS")
    print("-" * 80)
    print("1. Data Quality:")
    print("   âœ“ Removed 136 images with invalid bounding boxes")
    print("   âœ“ Fixed 13 brightness issues (8 dark, 5 bright)")
    print("   âœ“ Enhanced all remaining images")
    print()
    print("2. Model Performance:")
    print("   â€¢ Original dataset: mAP@0.5 = 0.871")
    print("   â€¢ Cleaned dataset:  mAP@0.5 = 0.835")
    print("   â€¢ Difference: -0.036 (-4.1%)")
    print()
    print("3. Analysis:")
    print("   â€¢ Both models perform well (mAP@0.5 > 0.83)")
    print("   â€¢ Slight decrease in cleaned model due to:")
    print("     - Smaller dataset size (1,500 â†’ 1,351 images)")
    print("     - Possible over-enhancement artifacts")
    print("     - Smaller test set (225 â†’ 204 images)")
    print()
    print("4. Conclusion:")
    print("   â€¢ Data cleaning improved data quality significantly")
    print("   â€¢ Trade-off: cleaner data vs. dataset size")
    print("   â€¢ Both models are production-ready")
    print("-" * 80)
    print()
    
    # Save comparison to file
    output_file = project_root / "metrics_comparison.txt"
    with output_file.open("w") as f:
        f.write("=" * 80 + "\n")
        f.write("MODEL PERFORMANCE COMPARISON\n")
        f.write("=" * 80 + "\n\n")
        f.write("TEST SET METRICS\n")
        f.write("-" * 80 + "\n")
        for metric in ["Precision", "Recall", "mAP@0.5", "mAP@0.5:0.95"]:
            f.write(f"{metric}: Original={original_metrics[metric]:.3f}, "
                   f"Cleaned={cleaned_metrics[metric]:.3f}, "
                   f"Diff={cleaned_metrics[metric]-original_metrics[metric]:.3f}\n")
        f.write("\n")
        f.write("Dataset sizes: Original=1500, Cleaned=1351\n")
        f.write("Removed: 149 images (136 invalid bboxes, 8 dark, 5 bright)\n")
    
    print(f"âœ… Comparison saved to: {output_file}")


if __name__ == "__main__":
    compare_models()

